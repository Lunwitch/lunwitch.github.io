---
layout: single
title:  "Sequence-to-sequence"
---

# 본 글은 [딥러닝을 이용한 자연어 처리 입문]과 PARK JI HO님의 [위클리 NLP]를 바탕으로 하여 작성하였습니다.


## <시퀀스-투-시퀀스(이하 Seq2seq) 이해하기>


번역기에서 사용되는 모델입니다.

![image](https://user-images.githubusercontent.com/108162891/197916325-2a56d2ca-093b-45a5-93ff-a9e1ece5c1dd.png)

<seq2seq 모델 내부의 모습>

![image](https://user-images.githubusercontent.com/108162891/197916358-0c87c10a-1491-424a-82db-db9765d85cb5.png)


인코더와 디코더라는 두 개의 모듈로 구성됩니다. 인코더는 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤에 이 단어 모든 정보들을 압축해서 하나의 만들게 되는데,
이를 컨텍스트 벡터라고 합니다.
입력 문장의 정보가 하나의 컨텍스트 벡터로 모두 압축되면 인코더는 컨텍스트 벡터를 디코더로 전송합니다. 디코더는 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력합니다.

![image](https://user-images.githubusercontent.com/108162891/197916675-93b0c1ac-d9cd-48a9-baa7-31a00377ed88.png)

입력 문장을 받는 RNN 셀을 인코더, 출력 문장을 출력하는 RNN 셀을 디코더라고 합니다.
인코더 RNN 셀의 마지막 시점의 은닉 상태를 디코더 RNN 셀로 넘겨주는데 이를 컨텍스트 벡터라고 합니다. 컨텍스트 벡터는 디코더 RNN 셀의 첫번째 은닉 상태에 사용됩니다.

인코더 RNN은 번역해야 할 문장을 한 단어씩 읽습니다. 하나의 단어가 들어갈 때마다 RNN의 hidden state가 바뀝니다.이란?

디코더는 초기 입력으로 문장의 시작을 의미하는 심볼 <sos>가 들어갑니다. 디코더는 <sos>가 입력되면, 다음에 등장할 확률이 높은 단어를 예측합니다.
  
텍스트를 벡터로 바꾸는 방법은 주로 워드 임베딩이 사용됩니다. 예를 들어,
  
![image](https://user-images.githubusercontent.com/108162891/197916973-fe3ed6d0-0840-4f4a-aef1-d20d0b690c21.png)

  I, am, a, student 라는 단어들에 대한 임베딩 벡터는 위와 같은 모습을 가집니다.
  
  
  
  
  ## RNN이란?
  
사람이 읽는 것처럼 문장의 단어들을 순차적으로 처리하는 neural network 모델입니다. Recurrent Neural Network(RNN)입니다.
RNN은 NLP 이외의 time series modelling에 굉장히 많이 쓰이는 모델입니다. 매 초마다의 주식 거래, 매 시간의 기온 등 각 시가마다 바뀌는 값이 있는 데이터를 time series라고 합니다.
input 여러 개가 순차적으로 들어가고, output이 하나씩 생성됩니다.
단어 한 개를 하나의 input으로 볼 수 있습니다. 각 단어가 컨베이어 벨트를 타고 RNN으로 한 개씩 input 된다는 것을 알 수 있습니다.
  
  
  ## LSTM이란?
  
  RNN과 LSTM의 가장 큰 차이점은 모델 안에 gate를 추가하여 input, output 그리고 memory 간 흐르는 정보를 섬세하게 제어했다는 점입니다.
  LSTM에는 input gate, output gate, forget gate, update gate가 있습니다. 어떤 정보를 받아들이고, 내보내고, 잊어버리고, 저장할 지를 학습하는 수학 모델입니다.
  
  
  
